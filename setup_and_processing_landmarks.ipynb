{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY= os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "# Define base data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "\n",
    "# Create temporary directories for extracted files\n",
    "LANDMARKS_DIR = os.path.join(DATA_DIR, 'landmarks_extracted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for images in html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the data into DataFrames would be a good approach for several reasons:\n",
    " 1. Easy data manipulation and analysis\n",
    " 2. Built-in methods for handling missing values\n",
    " 3. Efficient filtering and sorting\n",
    " 4. Simple integration with other data processing libraries\n",
    " 5. Good for structured data representation\n",
    "\n",
    "-  We can create DataFrames for each type of data:\n",
    "    - landmarks_df: to store landmark information\n",
    "    - municipalities_df: to store municipality data \n",
    "    - news_df: to store news articles\n",
    "\n",
    "- This will make it easier to:\n",
    "    - Clean and preprocess the data\n",
    "    - Extract relevant features\n",
    "    - Prepare data for vector embeddings\n",
    "    - Track metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The probelm with the landmark name is that sometimes it contain information about the town (like its name) and sometimes it doesn't.\n",
    "\n",
    "- I might need to group the landmarks by town and then clean the names of the landmarks. How could i do this if the landmark_name not all have the town name. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BEAUTIFULSOUP TO SCRAPE INFORMATION FORM THE landmark_texts[html] TO get the coordinates and location. \n",
    "\n",
    "- Get matadata from HTML using Beautifulsoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Scrapping for towns, coordinates, contexts, key features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corrected Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lOAD DATASET\n",
    "landmarks_df_corr = pd.read_csv('data/Landmark_Processd/landmarks_corrected_GPT3_LASTEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE KEY FEATURE AND IMAGES\n",
    "landmarks_df_corr = landmarks_df_corr.drop(['key_features', 'images'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_df_corr.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(landmarks_df_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scape for images in html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_html(html_content: str, landmark_name: str) -> Dict:\n",
    "    \"\"\"Extract images and their descriptions from Wikipedia HTML content\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content\n",
    "        landmark_name (str): Name of the landmark for reference\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing landmark name and list of image information\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    result = {\n",
    "        'name': landmark_name,\n",
    "        'images': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Find all image containers (figures and divs)\n",
    "        image_containers = soup.find_all(['figure', 'div'], class_=['thumb', 'image', 'thumbinner'])\n",
    "        \n",
    "        # Method 2: Find all img tags directly\n",
    "        img_tags = soup.find_all('img')\n",
    "        \n",
    "        # Method 3: Find gallery sections\n",
    "        galleries = soup.find_all(['ul', 'div'], class_=['gallery', 'gallery mw-gallery-traditional'])\n",
    "        \n",
    "        # Process containers\n",
    "        for container in image_containers:\n",
    "            img = container.find('img')\n",
    "            if img:\n",
    "                image_data = process_image(img, container)\n",
    "                if image_data:\n",
    "                    result['images'].append(image_data)\n",
    "        \n",
    "        # Process direct img tags\n",
    "        for img in img_tags:\n",
    "            if not img.parent.name in ['figure', 'div'] or not any(c in img.parent.get('class', []) for c in ['thumb', 'image', 'thumbinner']):\n",
    "                image_data = process_image(img)\n",
    "                if image_data:\n",
    "                    result['images'].append(image_data)\n",
    "        \n",
    "        # Process galleries\n",
    "        for gallery in galleries:\n",
    "            gallery_images = gallery.find_all('img')\n",
    "            for img in gallery_images:\n",
    "                image_data = process_image(img)\n",
    "                if image_data:\n",
    "                    result['images'].append(image_data)\n",
    "        \n",
    "        # Remove duplicates based on URL\n",
    "        seen_urls = set()\n",
    "        unique_images = []\n",
    "        for img in result['images']:\n",
    "            if img['url'] not in seen_urls:\n",
    "                seen_urls.add(img['url'])\n",
    "                unique_images.append(img)\n",
    "        result['images'] = unique_images\n",
    "        \n",
    "        # If no images found, add placeholder\n",
    "        if not result['images']:\n",
    "            result['images'].append({\n",
    "                'url': 'None found',\n",
    "                'width': 'N/A',\n",
    "                'height': 'N/A',\n",
    "                'alt_text': 'No images available for this landmark',\n",
    "                'caption': 'No images found'\n",
    "            })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting images for {landmark_name}: {str(e)}\")\n",
    "        result['images'].append({\n",
    "            'url': 'Error occurred',\n",
    "            'width': 'N/A',\n",
    "            'height': 'N/A',\n",
    "            'alt_text': f'Error processing images: {str(e)}',\n",
    "            'caption': 'Error occurred while processing'\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_image(img, container=None):\n",
    "    \"\"\"Helper function to process individual images\"\"\"\n",
    "    try:\n",
    "        # Get image URL\n",
    "        src = img.get('src', '')\n",
    "        if src.startswith('//'):\n",
    "            src = 'https:' + src\n",
    "        \n",
    "        # Fix duplicate path segments in Wikipedia URLs\n",
    "        if '/wikipedia/commons/' in src:\n",
    "            # Split URL at /commons/ and take everything after it\n",
    "            base_parts = src.split('/commons/')\n",
    "            if len(base_parts) > 1:\n",
    "                # Remove any duplicate filename at the end\n",
    "                file_path = base_parts[1]\n",
    "                if '/' in file_path:\n",
    "                    # Keep only the first occurrence of the filename\n",
    "                    file_segments = file_path.split('/')\n",
    "                    unique_segments = []\n",
    "                    seen = set()\n",
    "                    for segment in file_segments:\n",
    "                        if segment not in seen:\n",
    "                            unique_segments.append(segment)\n",
    "                            seen.add(segment)\n",
    "                    file_path = '/'.join(unique_segments)\n",
    "                src = f\"https://upload.wikimedia.org/wikipedia/commons/{file_path}\"\n",
    "        \n",
    "        # Get image dimensions\n",
    "        width = img.get('width', '')\n",
    "        height = img.get('height', '')\n",
    "        \n",
    "        # Get alt text\n",
    "        alt_text = img.get('alt', '')\n",
    "        \n",
    "        # Look for caption\n",
    "        caption = None\n",
    "        if container:\n",
    "            caption_elem = container.find(['figcaption', 'div'], class_=['thumbcaption', 'caption'])\n",
    "            if caption_elem:\n",
    "                caption = caption_elem.get_text(strip=True)\n",
    "        \n",
    "        # Only add if it's a content image (skip icons, thumbnails, etc.)\n",
    "        if (not width or not height) or (int(width) > 50 and int(height) > 50):\n",
    "            if not src.endswith(('.svg', '.gif')) and 'icon' not in src.lower():\n",
    "                return {\n",
    "                    'url': src,\n",
    "                    'width': width or 'unknown',\n",
    "                    'height': height or 'unknown',\n",
    "                    'alt_text': alt_text,\n",
    "                    'caption': caption\n",
    "                }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Process landmarks\n",
    "landmarks_images_data = []\n",
    "\n",
    "print(\"Processing landmarks for images...\")\n",
    "for filename in os.listdir(LANDMARKS_DIR):\n",
    "    if filename.endswith('.txt'):\n",
    "        landmark_name = (filename.replace('.txt', '')\n",
    "                        .replace('(', '')\n",
    "                        .replace(')', '')\n",
    "                        .replace(',', '')\n",
    "                        .replace('-', '_')\n",
    "                        .lower())\n",
    "        \n",
    "        file_path = os.path.join(LANDMARKS_DIR, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                html_content = file.read()\n",
    "                images_info = extract_images_from_html(html_content, landmark_name)\n",
    "                landmarks_images_data.append(images_info)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "landmarks_images_df = pd.DataFrame(landmarks_images_data)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nDataFrame Shape:\", landmarks_images_df.shape)\n",
    "print(\"\\nSample landmarks with images:\")\n",
    "print(f\"Total landmarks processed: {len(landmarks_images_df)}\")\n",
    "print(\"\\nSample of first landmark's images:\")\n",
    "if not landmarks_images_df.empty:\n",
    "    first_landmark = landmarks_images_df.iloc[0]\n",
    "    print(f\"\\nLandmark: {first_landmark['name']}\")\n",
    "    print(f\"Number of images: {len(first_landmark['images'])}\")\n",
    "    if first_landmark['images']:\n",
    "        print(\"\\nFirst image details:\")\n",
    "        print(json.dumps(first_landmark['images'][0], indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "#landmarks_images_df.to_csv('processed_landmarks_images.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_images_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add images column from landmark_images_df to landmarks_df\n",
    "landmarks_df_corr_final = landmarks_df_corr.merge(\n",
    "    landmarks_images_df[['name', 'images']], \n",
    "    on='name',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_df_corr_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final landmarks DataFrame to CSV\n",
    "#landmarks_df_corr_final.to_csv('landmarks_with_imagesgpt3_latest.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot of primary and secondary categories\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot primary categories\n",
    "primary_counts = landmarks_df_corr['primary_category'].value_counts()\n",
    "sns.barplot(x=primary_counts.values, y=primary_counts.index, ax=ax1)\n",
    "ax1.set_title('Primary Categories')\n",
    "ax1.set_xlabel('Count')\n",
    "\n",
    "# Plot secondary categories \n",
    "secondary_counts = landmarks_df_corr['secondary_category'].value_counts()\n",
    "sns.barplot(x=secondary_counts.values, y=secondary_counts.index, ax=ax2)\n",
    "ax2.set_title('Secondary Categories')\n",
    "ax2.set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New DATASETS \n",
    "- Cultural Events\n",
    "- Restaurants\n",
    "- Hotels and Airbnbs\n",
    "- Para la naturaleza and DRNA webpage\n",
    "- Events\n",
    "\n",
    "# Concenrs\n",
    "1) Do I need to spend time on a code to scrape the websites?\n",
    "2) Since is just for the proyect, focus on finding a static list of datasets (Hotels,Airbnb, Resutarunts and Events)\n",
    "\n",
    "- I will have to focus on giving a lot of good promts so I can always have the control of the conversation when the user interacts with the chatbot.\n",
    "- If i dont find a good dataset of hotels and Airbnb, make sure the promts is setup to provide the user a link to booking and airbnb.\n",
    "- Similar to events and activities, go to \"https://app.voyturisteando.com/directorio\"\n",
    "- Provide similar tools that can help him improve the planning.https://mapa.plateapr.com/\n",
    "- Can can agent help look for nearby spot accounting on the location of the user? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_df_corr_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_df_corr_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Nans - csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    \"\"\"\n",
    "    Fill missing or blank values in DataFrame with specific placeholders\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    filled_df = df.copy()\n",
    "    \n",
    "    # Define placeholder values for each column\n",
    "    placeholders = {\n",
    "        'name': \"Unnamed Landmark\",\n",
    "        'town': \"Location to be verified\",\n",
    "        'latitude': 0.0,\n",
    "        'longitude': 0.0,\n",
    "        'content': \"No detailed description available\",\n",
    "        'direction': \"N/A\",\n",
    "        'primary_category': \"Uncategorized\",\n",
    "        'secondary_category': \"General\",\n",
    "        'website': \"No website listed\",\n",
    "        'visit_duration': \"Visit duration varies\",\n",
    "        'hours': \"Contact location for current hours\",\n",
    "        'admission': \"Contact location for current prices\",\n",
    "        'chatbot_tags': str([\"needs_update\"]),  # Convert to string representation\n",
    "        'images': str([])  # Convert to string representation\n",
    "    }\n",
    "    \n",
    "    # Fill missing values column by column\n",
    "    for column in filled_df.columns:\n",
    "        if column in ['latitude', 'longitude']:\n",
    "            # Handle numeric columns\n",
    "            filled_df[column] = pd.to_numeric(filled_df[column], errors='coerce').fillna(placeholders[column])\n",
    "        else:\n",
    "            # Handle string columns (including string representations of lists/dicts)\n",
    "            filled_df[column] = filled_df[column].fillna(placeholders[column])\n",
    "    \n",
    "    return filled_df\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "cleaned_landmarks_df = fill_missing_values(landmarks_df_corr_final)\n",
    "\n",
    "# Print summary of changes\n",
    "print(\"\\nMissing values before cleaning:\")\n",
    "print(landmarks_df_corr_final.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(cleaned_landmarks_df.isnull().sum())\n",
    "\n",
    "# Display a sample to verify the changes\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "print(cleaned_landmarks_df.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_landmarks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save cleaned DataFrame\n",
    "#cleaned_landmarks_df.to_csv('landmarks_with_imagesgpt3_latest_fill_NANS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize data for VectorData Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_landmarks_data(landmarks_df):\n",
    "    \"\"\"Convert landmarks DataFrame into structured format optimized for vector database\"\"\"\n",
    "    \n",
    "    structured_data = []\n",
    "    \n",
    "    for _, row in landmarks_df.iterrows():\n",
    "        try:\n",
    "            # Handle chatbot_tags - convert string representation to list\n",
    "            if isinstance(row['chatbot_tags'], str):\n",
    "                # Remove any extra quotes and convert to list\n",
    "                tags_str = row['chatbot_tags'].replace('\"', '').replace('[', '').replace(']', '')\n",
    "                chatbot_tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]\n",
    "            else:\n",
    "                chatbot_tags = []\n",
    "            \n",
    "            # Handle images - ensure it's a list\n",
    "            if isinstance(row['images'], str):\n",
    "                try:\n",
    "                    landmark_images = eval(row['images'])\n",
    "                except:\n",
    "                    landmark_images = []\n",
    "            else:\n",
    "                landmark_images = row['images'] if isinstance(row['images'], list) else []\n",
    "            \n",
    "            # Get up to 3 image URLs for the text embedding\n",
    "            image_urls = []\n",
    "            for img in landmark_images[:3]:  # Limit to first 3 images\n",
    "                if isinstance(img, dict) and 'url' in img:\n",
    "                    image_urls.append(img['url'])\n",
    "            \n",
    "            # Create image URLs text section\n",
    "            image_urls_text = \"\\n    \".join(image_urls) if image_urls else \"No images available\"\n",
    "            \n",
    "            # Create the structured entry\n",
    "            entry = {\n",
    "                'landmark_name': row['name'],\n",
    "                'coordinates': {\n",
    "                    'latitude': row['latitude'],\n",
    "                    'longitude': row['longitude']\n",
    "                },\n",
    "                'location': {\n",
    "                    'town': row['town'],\n",
    "                    'direction': row['direction']\n",
    "                },\n",
    "                'details': {\n",
    "                    'primary_category': row['primary_category'],\n",
    "                    'secondary_category': row['secondary_category'],\n",
    "                    'visit_duration': row['visit_duration'],\n",
    "                    'hours': row['hours'],\n",
    "                    'admission': row['admission'],\n",
    "                    'website': row['website']\n",
    "                },\n",
    "                'content': row['content'],\n",
    "                'images': landmark_images,\n",
    "                'metadata': {\n",
    "                    'has_images': len(landmark_images) > 0,\n",
    "                    'chatbot_tags': chatbot_tags\n",
    "                },\n",
    "                'text_for_embedding': f\"\"\"\n",
    "                    Landmark: {row['name']}\n",
    "                    \n",
    "                    Location: {row['town']}, {row['direction']} Puerto Rico\n",
    "                    Coordinates: Latitude {row['latitude']}, Longitude {row['longitude']}\n",
    "                    \n",
    "                    Category: {row['primary_category']} - {row['secondary_category']}\n",
    "                    \n",
    "                    Description: {row['content']}\n",
    "                    \n",
    "                    Visit Information:\n",
    "                    Duration: {row['visit_duration']}\n",
    "                    Hours: {row['hours']}\n",
    "                    Admission: {row['admission']}\n",
    "                    \n",
    "                    Website: {row['website']}\n",
    "                    \n",
    "                    Tags: {chatbot_tags}\n",
    "                    \n",
    "                    Images ({len(landmark_images)} available):\n",
    "                    {image_urls_text}\n",
    "                \"\"\".strip()\n",
    "            }\n",
    "            \n",
    "            structured_data.append(entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {row['name']}\")\n",
    "            print(f\"Error details: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured data\n",
    "structured_landmarks = create_structured_landmarks_data(cleaned_landmarks_df)\n",
    "\n",
    "# Print sample to verify\n",
    "print(f\"Processed {len(structured_landmarks)} landmarks\")\n",
    "print(\"\\nSample structured entry:\")\n",
    "print(json.dumps(structured_landmarks[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert structured data to DataFrame\n",
    "structured_df = pd.DataFrame(structured_landmarks)\n",
    "\n",
    "# Save to JSON for vector database processing\n",
    "#with open('processed_landmarks_gpt3_images_STRUCTURED_FILL_NANS.json', 'w', encoding='utf-8') as f:\n",
    "#    json.dump(structured_landmarks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "#structured_df.to_csv('processed_landmarks_gpt3_images_STRUCTURED_FILL_NANS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV\n",
    "structured_df.to_csv('processed_landmarks_gpt3_images_STRUCTURED_FILL_NANS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
