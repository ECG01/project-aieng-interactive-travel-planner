{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY= os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "# Define base data directory\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "# Define zip file paths\n",
    "#LANDMARKS_ZIP = os.path.join(DATA_DIR, 'landmarks.zip')\n",
    "#MUNICIPALITIES_ZIP = os.path.join(DATA_DIR, 'municipalities.zip')\n",
    "#NEWS_ZIP = os.path.join(DATA_DIR, 'elmundo_chunked_en_page1_15years.zip')\n",
    "\n",
    "# Create temporary directories for extracted files\n",
    "LANDMARKS_DIR = os.path.join(DATA_DIR, 'landmarks_extracted')\n",
    "MUNICIPALITIES_DIR = os.path.join(DATA_DIR, 'municipalities_extracted') \n",
    "NEWS_DIR = os.path.join(DATA_DIR, 'news_extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the data into DataFrames would be a good approach for several reasons:\n",
    " 1. Easy data manipulation and analysis\n",
    " 2. Built-in methods for handling missing values\n",
    " 3. Efficient filtering and sorting\n",
    " 4. Simple integration with other data processing libraries\n",
    " 5. Good for structured data representation\n",
    "\n",
    "-  We can create DataFrames for each type of data:\n",
    "    - landmarks_df: to store landmark information\n",
    "    - municipalities_df: to store municipality data \n",
    "    - news_df: to store news articles\n",
    "\n",
    "- This will make it easier to:\n",
    "    - Clean and preprocess the data\n",
    "    - Extract relevant features\n",
    "    - Prepare data for vector embeddings\n",
    "    - Track metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Initialize empty lists to store data\n",
    "landmark_names = []\n",
    "landmark_texts = []\n",
    "\n",
    "try:\n",
    "    for filename in os.listdir(LANDMARKS_DIR):\n",
    "        if filename.endswith('.txt'):  # Only process text files\n",
    "            file_path = os.path.join(LANDMARKS_DIR, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    html_content = file.read()\n",
    "                    landmark_names.append(filename[:-4])\n",
    "                    landmark_texts.append(html_content)\n",
    "            except PermissionError:\n",
    "                print(f\"Permission denied for file: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing directory {LANDMARKS_DIR}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The probelm with the landmark name is that sometimes it contain information about the town (like its name) and sometimes it doesn't.\n",
    "\n",
    "- I might need to group the landmarks by town and then clean the names of the landmarks. How could i do this if the landmark_name not all have the town name. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM LAT LON, IMAGE ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Load Municipality\n",
    "municipalities_df_enhanced = pd.read_csv('data/municipalities_extracted/Proccessed/municipalities_data_corr_gpt3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LETS PROCESS THE MUNICIPALITIES DATASET\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) LOOK FOR IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def extract_urls_from_html(html_content, base_url=\"https://en.wikipedia.org\"):\n",
    "    \"\"\"\n",
    "    Extract image and relevant content URLs from HTML content, limited to 2 per category\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content\n",
    "        base_url (str): Base URL to resolve relative URLs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing different types of extracted URLs (max 2 each)\n",
    "    \"\"\"\n",
    "    urls = {\n",
    "        'images': set(),  # Using sets to prevent duplicates\n",
    "        'content': set(), \n",
    "        'related_pages': set()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # 1. Find image URLs from img tags (max 2)\n",
    "        for img in soup.find_all('img'):\n",
    "            if len(urls['images']) >= 2:\n",
    "                break\n",
    "            src = img.get('src')\n",
    "            if src:\n",
    "                # Handle relative URLs\n",
    "                if src.startswith('/'):\n",
    "                    src = urljoin(base_url, src)\n",
    "                # Skip Wikipedia logo\n",
    "                if src == \"https://en.wikipedia.org/static/images/icons/wikipedia.png\":\n",
    "                    continue\n",
    "                if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                    urls['images'].add(src)\n",
    "        \n",
    "        # 2. Find meta content URLs (max 2)\n",
    "        for meta in soup.find_all('meta'):\n",
    "            if len(urls['content']) >= 2:\n",
    "                break\n",
    "            content = meta.get('content', '')\n",
    "            if 'upload.wikimedia.org' in content:\n",
    "                urls['content'].add(content)\n",
    "        \n",
    "        # 3. Find related wiki pages about Puerto Rico locations (max 2)\n",
    "        for link in soup.find_all('a'):\n",
    "            if len(urls['related_pages']) >= 2:\n",
    "                break\n",
    "            href = link.get('href', '')\n",
    "            if href.startswith('/wiki/') and 'Puerto_Rico' in href:\n",
    "                full_url = urljoin(base_url, href)\n",
    "                urls['related_pages'].add(full_url)\n",
    "            elif 'wikipedia.org' in href and 'Puerto_Rico' in href:\n",
    "                urls['related_pages'].add(href)\n",
    "        \n",
    "        # Convert sets back to lists for JSON serialization\n",
    "        return {k: list(v) for k, v in urls.items()}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing HTML: {str(e)}\")\n",
    "        return {k: [] for k in ['images', 'content', 'related_pages']}\n",
    "\n",
    "def process_municipality_files(municipalities_path):\n",
    "    \"\"\"\n",
    "    Process all municipality files and extract URLs\n",
    "    \n",
    "    Args:\n",
    "        municipalities_path (str): Path to the municipalities directory\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with municipality names as keys and extracted URLs as values\n",
    "    \"\"\"\n",
    "    municipality_urls = {}\n",
    "    \n",
    "    for filename in os.listdir(municipalities_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            municipality_name = filename.replace('.txt', '')\n",
    "            try:\n",
    "                with open(os.path.join(municipalities_path, filename), 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    urls = extract_urls_from_html(content)\n",
    "                    municipality_urls[municipality_name] = urls\n",
    "                    print(f\"Processed {municipality_name}: Found {sum(len(v) for v in urls.values())} URLs\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "    \n",
    "    return municipality_urls\n",
    "\n",
    "def create_urls_dataframe(municipality_urls):\n",
    "    \"\"\"\n",
    "    Convert the municipality URLs dictionary into a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        municipality_urls (dict): Dictionary containing URLs for each municipality\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with municipality URLs\n",
    "    \"\"\"\n",
    "    # Create lists to store the data\n",
    "    rows = []\n",
    "    \n",
    "    # Keep track of seen URLs to avoid duplicates\n",
    "    seen_urls = set()\n",
    "    \n",
    "    for municipality, urls in municipality_urls.items():\n",
    "        # For each image URL\n",
    "        for img_url in urls['images']:\n",
    "            if img_url not in seen_urls:\n",
    "                rows.append({\n",
    "                    'municipality_name': municipality,\n",
    "                    'url_type': 'image',\n",
    "                    'url': img_url\n",
    "                })\n",
    "                seen_urls.add(img_url)\n",
    "        \n",
    "        # For each content URL\n",
    "        for content_url in urls['content']:\n",
    "            if content_url not in seen_urls:\n",
    "                rows.append({\n",
    "                    'municipality_name': municipality,\n",
    "                    'url_type': 'content',\n",
    "                    'url': content_url\n",
    "                })\n",
    "                seen_urls.add(content_url)\n",
    "            \n",
    "        # For each related page URL\n",
    "        for page_url in urls['related_pages']:\n",
    "            if page_url not in seen_urls:\n",
    "                rows.append({\n",
    "                    'municipality_name': municipality,\n",
    "                    'url_type': 'related_page',\n",
    "                    'url': page_url\n",
    "                })\n",
    "                seen_urls.add(page_url)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process files and create DataFrame\n",
    "municipalities_path = 'data/municipalities_extracted'\n",
    "municipality_urls = process_municipality_files(municipalities_path)\n",
    "\n",
    "# Create DataFrame\n",
    "urls_df = create_urls_dataframe(municipality_urls)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "#urls_df.to_csv('data/municipality_urls.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_dataframe(municipalities_df_enhanced: pd.DataFrame, urls_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a structured DataFrame combining all municipality information\n",
    "    \n",
    "    Args:\n",
    "        municipalities_df_enhanced: DataFrame with enhanced municipality info (GPT processed)\n",
    "        urls_df: DataFrame with image URLs\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Structured DataFrame ready for vector storage\n",
    "    \"\"\"\n",
    "    # First, group image URLs by municipality and include both 'image' and 'content' types\n",
    "    image_urls = urls_df[\n",
    "        (urls_df['url_type'] == 'image') | (urls_df['url_type'] == 'content')\n",
    "    ].groupby('municipality_name')['url'].agg(list).reset_index()\n",
    "    \n",
    "    # Create the structured DataFrame\n",
    "    structured_data = []\n",
    "    \n",
    "    for _, row in municipalities_df_enhanced.iterrows():\n",
    "        # Get images for this municipality\n",
    "        municipality_images = image_urls[\n",
    "            image_urls['municipality_name'] == row['municipality_name']\n",
    "        ]['url'].tolist() if not image_urls[\n",
    "            image_urls['municipality_name'] == row['municipality_name']\n",
    "        ].empty else []\n",
    "        \n",
    "        # Filter out non-image URLs and duplicates\n",
    "        filtered_images = []\n",
    "        for url_item in municipality_images:\n",
    "            # Handle case where url_item might be a list\n",
    "            if isinstance(url_item, list):\n",
    "                for url in url_item:\n",
    "                    if isinstance(url, str) and any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                        filtered_images.append(url)\n",
    "            elif isinstance(url_item, str) and any(ext in url_item.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                filtered_images.append(url_item)\n",
    "                \n",
    "        municipality_images = list(set(filtered_images))\n",
    "        \n",
    "        # Create the structured entry\n",
    "        entry = {\n",
    "            'municipality_name': row['municipality_name'],\n",
    "            'coordinates': {\n",
    "                'latitude': row['latitude'],\n",
    "                'longitude': row['longitude']\n",
    "            },\n",
    "            'summary': row['content'],  # GPT-generated summary\n",
    "            'images': municipality_images,\n",
    "            'metadata': {\n",
    "                'has_images': len(municipality_images) > 0,\n",
    "                'coordinates_valid': row['coordinates_valid'],\n",
    "                'google_maps_url': row['google_maps_url']\n",
    "            },\n",
    "            'text_for_embedding': f\"\"\"\n",
    "                Municipality: {row['municipality_name']}\n",
    "                \n",
    "                Location: Latitude {row['latitude']}, Longitude {row['longitude']}\n",
    "                \n",
    "                Description: {row['content']}\n",
    "                \n",
    "                Images Available: {len(municipality_images)}\n",
    "                \n",
    "                Google Maps: {row['google_maps_url']}\n",
    "            \"\"\".strip()\n",
    "        }\n",
    "        \n",
    "        structured_data.append(entry)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    structured_df = pd.DataFrame(structured_data)\n",
    "    \n",
    "    # Save both CSV and JSON versions\n",
    "    output_dir = 'data'\n",
    "    \n",
    "    # Save as CSV (flattened version for easy viewing)\n",
    "    flat_data = []\n",
    "    for entry in structured_data:\n",
    "        flat_entry = {\n",
    "            'municipality_name': entry['municipality_name'],\n",
    "            'latitude': entry['coordinates']['latitude'],\n",
    "            'longitude': entry['coordinates']['longitude'],\n",
    "            'summary': entry['summary'],\n",
    "            'image_urls': entry['images'],  # Keep as list\n",
    "            'has_images': entry['metadata']['has_images'],\n",
    "            'coordinates_valid': entry['metadata']['coordinates_valid'],\n",
    "            'google_maps_url': entry['metadata']['google_maps_url'],\n",
    "            'text_for_embedding': entry['text_for_embedding']\n",
    "        }\n",
    "        flat_data.append(flat_entry)\n",
    "    \n",
    "    pd.DataFrame(flat_data).to_csv(f'{output_dir}/municipalities_structured.csv', index=False)\n",
    "    \n",
    "    # Save as JSON (preserving nested structure)\n",
    "    with open(f'{output_dir}/municipalities_structured.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return structured_df\n",
    "\n",
    "# Create the structured DataFrame\n",
    "structured_df = create_structured_dataframe(municipalities_df_enhanced, urls_df)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nStructured DataFrame Shape:\", structured_df.shape)\n",
    "print(\"\\nSample of first municipality's images:\")\n",
    "print(f\"\\nMunicipality: {structured_df.iloc[0]['municipality_name']}\")\n",
    "print(f\"Number of images: {len(structured_df.iloc[0]['images'])}\")\n",
    "print(\"\\nImage URLs:\")\n",
    "for url in structured_df.iloc[0]['images']:\n",
    "    print(f\"- {url}\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total municipalities: {len(structured_df)}\")\n",
    "print(f\"Average images per municipality: {structured_df['images'].apply(len).mean():.1f}\")\n",
    "print(f\"Municipalities with images: {structured_df['metadata'].apply(lambda x: x['has_images']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORGANIZE DATA FOR VECTORDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_dataframe(municipalities_df_enhanced: pd.DataFrame, urls_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a structured DataFrame combining all municipality information\n",
    "    \n",
    "    Args:\n",
    "        municipalities_df_enhanced: DataFrame with enhanced municipality info (GPT processed)\n",
    "        urls_df: DataFrame with image URLs\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Structured DataFrame ready for vector storage\n",
    "    \"\"\"\n",
    "    # First, group image URLs by municipality and include both 'image' and 'content' types\n",
    "    image_urls = urls_df[\n",
    "        (urls_df['url_type'] == 'image') | (urls_df['url_type'] == 'content')\n",
    "    ].groupby('municipality_name')['url'].agg(list).reset_index()\n",
    "    \n",
    "    # Create the structured DataFrame\n",
    "    structured_data = []\n",
    "    \n",
    "    for _, row in municipalities_df_enhanced.iterrows():\n",
    "        # Get images for this municipality\n",
    "        municipality_images = image_urls[\n",
    "            image_urls['municipality_name'] == row['municipality_name']\n",
    "        ]['url'].tolist() if not image_urls[\n",
    "            image_urls['municipality_name'] == row['municipality_name']\n",
    "        ].empty else []\n",
    "        \n",
    "        # Filter out non-image URLs and duplicates\n",
    "        filtered_images = []\n",
    "        for url_item in municipality_images:\n",
    "            # Handle case where url_item might be a list\n",
    "            if isinstance(url_item, list):\n",
    "                for url in url_item:\n",
    "                    if isinstance(url, str) and any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                        filtered_images.append(url)\n",
    "            elif isinstance(url_item, str) and any(ext in url_item.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif']):\n",
    "                filtered_images.append(url_item)\n",
    "                \n",
    "        municipality_images = list(set(filtered_images))\n",
    "        \n",
    "        # Create the structured entry\n",
    "        entry = {\n",
    "            'municipality_name': row['municipality_name'],\n",
    "            'coordinates': {\n",
    "                'latitude': row['latitude'],\n",
    "                'longitude': row['longitude']\n",
    "            },\n",
    "            'summary': row['content'],  # GPT-generated summary\n",
    "            'images': municipality_images,\n",
    "            'metadata': {\n",
    "                'has_images': len(municipality_images) > 0,\n",
    "                'coordinates_valid': row['coordinates_valid'],\n",
    "                'google_maps_url': row['google_maps_url']\n",
    "            },\n",
    "            'text_for_embedding': f\"\"\"\n",
    "                Municipality: {row['municipality_name']}\n",
    "                \n",
    "                Location: Latitude {row['latitude']}, Longitude {row['longitude']}\n",
    "                \n",
    "                Description: {row['content']}\n",
    "                \n",
    "                Images Available: {len(municipality_images)}\n",
    "                \n",
    "                Google Maps: {row['google_maps_url']}\n",
    "            \"\"\".strip()\n",
    "        }\n",
    "        \n",
    "        structured_data.append(entry)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    structured_df = pd.DataFrame(structured_data)\n",
    "    \n",
    "    # Save both CSV and JSON versions\n",
    "    output_dir = 'data'\n",
    "    \n",
    "    # Save as CSV (flattened version for easy viewing)\n",
    "    flat_data = []\n",
    "    for entry in structured_data:\n",
    "        flat_entry = {\n",
    "            'municipality_name': entry['municipality_name'],\n",
    "            'latitude': entry['coordinates']['latitude'],\n",
    "            'longitude': entry['coordinates']['longitude'],\n",
    "            'summary': entry['summary'],\n",
    "            'image_urls': entry['images'],  # Keep as list\n",
    "            'has_images': entry['metadata']['has_images'],\n",
    "            'coordinates_valid': entry['metadata']['coordinates_valid'],\n",
    "            'google_maps_url': entry['metadata']['google_maps_url'],\n",
    "            'text_for_embedding': entry['text_for_embedding']\n",
    "        }\n",
    "        flat_data.append(flat_entry)\n",
    "    \n",
    "    pd.DataFrame(flat_data).to_csv(f'{output_dir}/municipalities_structured.csv', index=False)\n",
    "    \n",
    "    # Save as JSON (preserving nested structure)\n",
    "    with open(f'{output_dir}/municipalities_structured.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return structured_df\n",
    "\n",
    "# Create the structured DataFrame\n",
    "structured_df = create_structured_dataframe(municipalities_df_enhanced, urls_df)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nStructured DataFrame Shape:\", structured_df.shape)\n",
    "print(\"\\nSample of first municipality's images:\")\n",
    "print(f\"\\nMunicipality: {structured_df.iloc[0]['municipality_name']}\")\n",
    "print(f\"Number of images: {len(structured_df.iloc[0]['images'])}\")\n",
    "print(\"\\nImage URLs:\")\n",
    "for url in structured_df.iloc[0]['images']:\n",
    "    print(f\"- {url}\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nStatistics:\")\n",
    "print(f\"Total municipalities: {len(structured_df)}\")\n",
    "print(f\"Average images per municipality: {structured_df['images'].apply(len).mean():.1f}\")\n",
    "print(f\"Municipalities with images: {structured_df['metadata'].apply(lambda x: x['has_images']).sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
